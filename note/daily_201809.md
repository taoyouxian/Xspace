## 0916
### zookeeper & etcd
- 分布式锁/缓存
- 调研（PPT）

### netty & dubbo


### casechain
- novel idea
- 具体合作


## 0917
### valatile变量
[如何理解](https://www.ibm.com/developerworks/cn/java/j-jtp06197.html)

### ObjectEncoder & ObjectDecoder

1. update pixels client and server

2. Serialzable接口的POJO对象

3. ObjectDecoder 

   ```
   ObjectDecoder负责对实现了Serialzable接口的POJO对象进行解码，它有多个构造函数，支持不同的ClassResolver。 
   服务端使用的是weakCachingConcurrentResolver，创建线程安全的WeakReferenceMap对类加载器进行缓存，它支持多线程并发访问。当虚拟机内存不足时，会释放缓存中的内存。 
   客户端使用的是cacheDisabled，禁止对类加载器进行缓存，它在基于OSGi的动态模块化编程中经常使用。由于OSGi的bundle可以进行热部署和热升级，当某个bundle升级后，它对应的类加载器也将一起升级，因此在动态模块化编程中，很少对类加载器进行缓存，因为它随时可能发生变化。 
   ```

   

# 0918

### Java序列化

```
我们使用Netty4本身自带的ObjectDecoder，ObjectEncoder来实现POJO对象的传输，但其使用的是Java内置的序列化，由于Java序列化的性能并不是很好，所以很多时候我们需要用其他高效的序列化方式，例如 protobuf，Hessian， Kryo，Jackson，fastjson等。
```

### 流式传输特点

```
In a stream-based transport such as TCP/IP, received data is stored into a socket receive buffer. Unfortunately, the buffer of a stream-based transport is not a queue of packets but a queue of bytes. It means, even if you sent two messages as two independent packets, an operating system will not treat them as two messages but as just a bunch of bytes. Therefore, there is no guarantee that what you read is exactly what your remote peer wrote. For example, let us assume that the TCP/IP stack of an operating system has received three packets: 
```

- 通常情况下有下面几种解决方案： 

  1. 消息定长
  2. 在包尾增加一个标识，通过这个标志符进行分割
  3. 将消息分为两部分，也就是消息头和消息尾，消息头中写入要发送数据的总长度，通常是在消息头的第一个字段使用int值（如果消息很大可以考虑用long值）来标识发送数据的长度

  

### Kryo对POJO对象进行序列化 

```
为了提高RPC的性能，增加了Kryo和FST两种高性能的序列化方式，基准测试报告地址：https://dangdangdotcom.github.io/dubbox/serialization.html
```



```
Kryo 实例的创建/初始化是相当昂贵的，所以在多线程的情况下，您应该线程池化 Kryo 实例。简单的解决方案是使用 ThreadLocal 将 Kryo实例绑定到 Threads。
```

[高性能NIO框架Netty](https://github.com/yinjihuan/netty-im)

```
高性能NIO框架Netty入门篇
高性能NIO框架Netty-对象传输
高性能NIO框架Netty-整合kryo高性能数据传输
高性能NIO框架Netty-整合Protobuf高性能数据传输
高性能NIO框架Netty-Netty4自带编解码器详解
Netty粘包拆包解决方案
Netty 断线重连解决方案
Netty 实现简单的HTTP服务
```



## 0919

### KryoSerializer

```
解决了“KryoException: Buffer underflow”和“Kryo NullPointerException:element”的问题， 但是测试了性能上， 时间会多一点， 就是在TestMetadataService.testGetTableLayouts()的地方.
```

- [实现](https://www.cnblogs.com/fanguangdexiaoyuer/p/6131042.html)
- [change kryo serde method usage ](https://github.com/cloudera/livy/commit/a685ecc9e07803bb4ef1660c1daff3dd0274e5cd)

Reference： [Netty Encoder And Decoder using Kryo Serialization](https://stackoverflow.com/questions/31147296/netty-encoder-and-decoder-using-kryo-serialization)



## 0921-0924

### 开小组会

- 全局的读写锁 `zookeeper` or `etcd`
- `note` 在*compact*时候应该注意的问题，小文件删除，和大文件创建，*SQL*在查询，会读取两种文件路径下面

的所有文件，会有读取不全和读取过多的问题

- 守护进程： 最好是无状态进程，即不需要记录状态和时间点（无执行上下文），可以在宕机后重新拉起来，继续工作

- 文件块和节点的关系，保证节点存储各自的列的缓存

- [*hadoop*的心跳机制](https://blog.csdn.net/lb812913059/article/details/78713523)

  - 心跳机制

    ```
    1) master启动的时候，会开启一个RPC server
    2) slave启动时进行连接master，并每隔3秒钟主动向master发送一个“心跳”
       将自己的状态信息告诉master，然后master通过这个心跳的返回值，向slave节点传达指令。
    ```

  - HDFS

    ```
    DataNode----->NameNode   3s 本地磁盘上block块的使用情况     1h block的report
    当长时间没有发送心跳时，NameNode就判断DataNode的连接已经中断,不能继续工作了
    就把他定性为”dead node”。NameNode会检查dead node中的副本数据，复制到其他的data node中。
    ```

  - YARN

    ```
    NodeManager----->ResourceManager    3s 本节点上cpu 内存.....
    ApplicationMaster----->ResourceManager  申请资源，返还资源
    ```

  - MapReduce

    ```
    TaskTracker----->JobTracker  汇报节点和任务运行状态信息
        1、判断Tasktracker是否活着 
        2、及时让Jobtracker获取各个节点上的资源使用情况和任务运行状态 
        3、为Tasktracker分配任务 
    
    注意：
        Jobtracker与Tasktracker之间采用了Pull（拉）而不是Push（推）模型
        Jobtracker不会主动向Tasktracker发送任何信息，而是由Tasktracker主动通过心跳领取属于自己的信息
        Jobtracker只能通过心跳应答的形式为各个Tasktracker分配任务
        Tasktracker周期性的调用RPC函数heartbeat向Jobtracker汇报信息和领取任务
    ```




## 0925-27

### ETCD

- Command

```
echo $ENDPOINTS
presto00:2379,presto01:2379,presto02:2379,presto03:2379,presto04:2379

etcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status
etcdctl --write-out=table --endpoints=$ENDPOINTS member list

etcdctl put /foo-service service
etcdctl put /foo-service/container1 localhost:1111
etcdctl get /foo-service
etcdctl put /foo-service/container2 localhost:2222
curl -X PUT http://127.0.0.1:2379/v2/keys/foo-service -d value="Hello"

curl -X PUT http://127.0.0.1:2379/v2/keys/etcd  -d value="helloworld"
curl http://127.0.0.1:2379/v2/keys/etcd
http://presto00:2379/v2/keys/etcd

http://presto00:2379/v2/keys/[key]
http://presto01:2379/v2/stats/leader
http://presto00:2379/v2/keys/?recursive=true
```

[etcd-api](https://coreos.com/etcd/docs/3.3.0/v2/api.html#using-a-directory-ttl)



### LSTM汇报

